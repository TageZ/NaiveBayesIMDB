{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"Homework2.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# pip install nltk\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import otter\n",
    "\n",
    "grader = otter.Notebook()\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(review):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    review = review.lower()\n",
    "\n",
    "    review = emoji_pattern.sub(r'', review)   \n",
    "\n",
    "    #Remove contractions\n",
    "    review=re.sub(\"isn't\", \"is not\", review)\n",
    "    review=re.sub(\"he's\", \"he is\", review)\n",
    "    review=re.sub(\"what's\", \"what is\", review)\n",
    "    review=re.sub(\"it's\", \"it is\", review)\n",
    "    review=re.sub(\"couldn't\", \"could not\", review)\n",
    "    review=re.sub(\"wouldn't\", \"would not\", review)\n",
    "    review=re.sub(\"can't\", \"cannot\", review)\n",
    "    review=re.sub(\"we're\", \"we are\", review)\n",
    "    review=re.sub(\"aren't\", \"are not\", review)\n",
    "    review=re.sub(\"i'm\", \"i am\", review)\n",
    "    review=re.sub(\"you're\", \"you are\", review)\n",
    "    review=re.sub(\"she's\", \"she is\", review)\n",
    "    review=re.sub(\"wasn't\", \"was not\", review)\n",
    "    review=re.sub(\"they're\", \"they are\", review)\n",
    "    review=re.sub(\"hasn't\", \"has not\", review)\n",
    "    review=re.sub(\"that's\", \"that is\", review)\n",
    "    review=re.sub(\"here's\", \"here is\", review)\n",
    "    review=re.sub(\"who's\", \"who is\", review)\n",
    "    review=re.sub(\"there's\", \"there is\", review)\n",
    "    review=re.sub(\"weren't\", \"were not\", review)\n",
    "    review=re.sub(\"haven't\", \"have not\", review)\n",
    "    review=re.sub(\"don't\", \"do not\", review)\n",
    "    review=re.sub(\"hadn't\", \"had not\", review)\n",
    "    review=re.sub(\"won't\", \"will not\", review)\n",
    "    review=re.sub(\"did't\", \"did not\", review)\n",
    "    review=re.sub(\"doesn't\", \"does not\", review)\n",
    "    review=re.sub(\"shouldn't\", \"should not\", review)\n",
    "\n",
    "    #Remove links\n",
    "    review = re.sub('https?://\\S+|www\\.\\S+', '', review)\n",
    "\n",
    "    #Remove non alphabetic characters\n",
    "    review = re.sub(r'\\n', ' ', review)\n",
    "    review = re.sub(r'[^a-zA-Z\\s]', '', review)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in and clean data\n",
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "cleaned_data = data['review'].apply(clean_data)\n",
    "\n",
    "cleaned_data = pd.DataFrame(cleaned_data)\n",
    "cleaned_data['sentiment'] = data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "# this will take a long time to run, likely at least 10 minutes.\n",
    "cleaned_data['no_sw'] = cleaned_data['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find value counts of all words in all data points\n",
    "counter = Counter()\n",
    "for text in cleaned_data[\"no_sw\"].values:\n",
    "    for word in text.split():\n",
    "        counter[word] += 1\n",
    "\n",
    "# creates a set of the 10 most common words amongst all reviews\n",
    "FREQWORDS = set([w for (w, wc) in counter.most_common(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_freqwords(text, freqwords):\n",
    "    '''Given a review that has been cleaned, returns the review without the most frequently appearing words'''\n",
    "    return ' '.join([word for word in text.split() if word not in (freqwords)])\n",
    "        \n",
    "# remove frequent words that do not add value to the model\n",
    "cleaned_data[\"no_sw\"] = cleaned_data[\"no_sw\"].apply(lambda text: remove_freqwords(text, FREQWORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace the sentiment with binary 0 or 1, prepare for model\n",
    "tokenized_data = cleaned_data.drop(columns=['review'])\n",
    "tokenized_data.columns = ['sentiment', 'review']\n",
    "\n",
    "#convert the sentiment values to a binary 0 or 1\n",
    "tokenized_data['sentiment'] = tokenized_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "#complete data preprocessing by changing the review format from a string of words to a list of strings containing individual words\n",
    "tokenized_data['review'] = tokenized_data['review'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Part 2: Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NaiveBayesModel:\n",
    "    '''Class representing the implementation of the Naive Bayes model'''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.occurrence_table = {}\n",
    "        self.probability_table = {}\n",
    "        self.labels = []\n",
    "\n",
    "    def train_model(self, data, labels):\n",
    "        '''Runs the training process for the model, building the occurrence table and probability table'''\n",
    "\n",
    "        self.labels = list(set(labels))\n",
    "        self.build_occurrence_table(data, labels)\n",
    "        self.build_probability_table()\n",
    "\n",
    "        pass\n",
    "\n",
    "    def build_occurrence_table(self, data, labels):\n",
    "        '''Private function to create the occurrence table given the training data and labels'''\n",
    "\n",
    "        # creates a nested dictionary table of frequencies based on the training data.\n",
    "        # example format for the word 'word': {'word': {0: 3, 1: 15}}\n",
    "\n",
    "        for review, sentiment in zip(data, labels):\n",
    "            for word in review:\n",
    "                if word not in self.occurrence_table:\n",
    "                    self.occurrence_table[word] = {0: 0, 1: 0}\n",
    "                \n",
    "                self.occurrence_table[word][sentiment] += 1\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def build_probability_table(self):\n",
    "        '''Private function to create the probability table based on the occurrence table'''\n",
    "\n",
    "        # generates a table of probabilities based on the frequencies recorded in the occurrence table\n",
    "        # example format for the word 'word': {'word': {0: 0.1667, 1: 0.8333}}\n",
    "\n",
    "        for word, sentiment_counts in self.occurrence_table.items():\n",
    "            total_occurrences = sum(sentiment_counts.values())\n",
    "\n",
    "            probabilities = {}\n",
    "            for sentiment, count in sentiment_counts.items():\n",
    "                probabilities[sentiment] = count / total_occurrences\n",
    "\n",
    "            self.probability_table[word] = probabilities\n",
    "\n",
    "        pass\n",
    "\n",
    "    def predict(self, variables):\n",
    "        '''Takes a set of variables, and predicts the class they should belong to'''\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        #TODO - implement prediction using the Naive Bayes method\n",
    "        # remember that if the model has the same probability of either class, it should pick randomly between the two\n",
    "\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform a train/test split on the data\n",
    "X=tokenized_data['review']\n",
    "y=tokenized_data['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=30)\n",
    "\n",
    "# train and predict using the Naive Bayes model implementation\n",
    "nb_model = NaiveBayesModel()\n",
    "nb_model.train_model(X_test, y_test)\n",
    "pred = nb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO - Create and display confusion matrices and evaluation metrics to prove the performance of your model.\n",
    "# required: accuracy, recall, f-score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "naivebayes",
   "tests": {
    "p1": {
     "name": "p1",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
